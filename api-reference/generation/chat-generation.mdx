---
title: "ChatGeneration"
---

A `ChatGeneration` contains all of the data that has been sent to a message-based LLM (like GPT-4) as well as the response from the LLM.
It is designed to be passed to a [Step](/concepts/step) to enable the [Prompt Playground](/advanced-features/prompt-playground).

## Attributes

<ParamField path="model" type="str" optional>
  The LLM model used.
</ParamField>

<ParamField path="message_completion" type="GenerationMessage" optional>
  The completion of the prompt. More info in
  [GenerationMessage](/api-reference/generation/generation-message).
</ParamField>

<ParamField path="variables" type="Dict[str, Any]" optional>
  The variables to the prompt, represented as a dictionary.
</ParamField>

<ParamField path="messages" type="List[GenerationMessage]" optional>
  The list of messages that form the prompt if in chat mode.
</ParamField>

<ParamField path="provider" type="str" optional>
  The provider of the LLM, such as "openai-chat".
</ParamField>

<ParamField path="tools" type="List[Dict]" optional>
  The list of tools that can be used by the LLM.
</ParamField>

<ParamField path="settings" type="Dict[str, Any]" optional>
  The settings the LLM provider was called with. Can include OpenAI tools and
  functions.
</ParamField>

<ParamField path="error" type="str" optional>
  The error returned by the LLM.
</ParamField>

<ParamField path="tags" type="str" optional>
  The tags you want to assign to this generation.
</ParamField>

<ParamField path="input_token_count" type="int" optional>
  Total tokens sent to the LLM.
</ParamField>

<ParamField path="output_token_count" type="int" optional>
  Total tokens returned by the LLM
</ParamField>

<ParamField path="token_count" type="int" optional>
  Total tokens used by the LLM.
</ParamField>

<ParamField path="tt_first_token" type="float" optional>
  Time from the request to receiving the first token.
</ParamField>

<ParamField path="token_throughput_in_s" type="float" optional>
  LLM speed of token generation, in tokens per second.
</ParamField>

<ParamField path="duration" type="float" optional>
  Total duration of the LLM request.
</ParamField>

## Example

```python
import os

from chainlit.playground.providers import ChatOpenAI
import chainlit as cl

# If no OPENAI_API_KEY is available, the ChatOpenAI provider won't be available in the prompt playground
os.environ["OPENAI_API_KEY"] = "sk-..."

template = """Hello, this is a template.
This is a variable1 {variable1}
And this is variable2 {variable2}
"""

variables = {
    "variable1": "variable1 value",
    "variable2": "variable2 value",
}

settings = {
    "model": "gpt-3.5-turbo",
    "temperature": 0,
    # ... more settings
}


@cl.step(type="llm")
async def fake_llm():
    # Pretend we're calling the LLM
    await cl.sleep(2)

    fake_completion = "The openai completion"

    prompt = template.format(**variables)

    # Add the generation to the current step
    cl.context.current_step.generation = cl.ChatGeneration(
        model=settings["model"],
        provider=ChatOpenAI.id,
        message_completion={
          "content": fake_completion, "role": "assistant"
        },
        variables=variables,
        settings=settings,
        messages=[
            {
                "content": "You are a helpful assistant.", "role": "system"
            },
            {
                "content": prompt, "role": "user"
            },
        ],
    )

    return fake_completion


@cl.on_chat_start
async def start():
    await fake_llm()
```

<Frame caption="A chat completion displayed in the Prompt Playground">
  <img src="/images/chat-mode-pp.png" />
</Frame>
