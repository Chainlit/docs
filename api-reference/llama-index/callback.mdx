---
title: "LlamaIndexCallbackHandler"
---

Callback Handler to enable Chainlit to display intermediate steps in the UI.

### Usage

```python Code Example
from llama_index.callbacks.base import CallbackManager
from llama_index import (
    LLMPredictor,
    ServiceContext,

)
from langchain.chat_models import ChatOpenAI

import chainlit as cl
from chainlit.llama_index.callbacks import LlamaIndexCallbackHandler


@cl.on_chat_start
async def factory():
    llm_predictor = LLMPredictor(
        llm=ChatOpenAI(
            temperature=0,
            model_name="gpt-3.5-turbo",
            streaming=True,
        ),
    )
    service_context = ServiceContext.from_defaults(
        llm_predictor=llm_predictor,
        chunk_size=512,
        callback_manager=CallbackManager([LlamaIndexCallbackHandler()]),
    )
```
