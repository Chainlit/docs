---
title: "LangChain Callback Handler"
---

When running your LangChain agent, it is essential to provide the appropriate callback handler. This ensures seamless communication between the Chainlit UI and your agent.

There are two ways to pass the callback handler:

1. Pass the callback handler during runtime. This will apply the handler to all calls made by your agent.
2. Pass the callback handler to individual components, such as an `llm`, allowing you to control what information is returned to the UI.

### Callback Handler

The following code example demonstrates how to pass a callback handler:

```python
llm = OpenAI(temperature=0)
llm_math = LLMMathChain.from_llm(llm=llm)

@cl.on_message
async def main(message: cl.Message):

    res = await llm_math.acall(message.content, callbacks=[cl.LangchainCallbackHandler()])

    await cl.Message(content="Hello").send()
```

### Final Answer streaming

If streaming is enabled at the LLM level, Langchain will only stream the intermediate steps. You can enable final answer streaming by passing `stream_final_answer=True` to the callback handler.

```python
# Optionally, you can also pass the prefix tokens that will be used to identify the final answer
answer_prefix_tokens=["FINAL", "ANSWER"]

cl.LangchainCallbackHandler(
        stream_final_answer=True,
        answer_prefix_tokens=answer_prefix_tokens,
    )
```

<Warning>
  Final answer streaming will only work with prompts that have a consistent
  final answer pattern. It will also not work with
  `AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`
</Warning>

### Conclusion

Choose the appropriate callback handler based on your specific requirements, ensuring seamless interaction between your LangChain agent and the Chainlit UI.
