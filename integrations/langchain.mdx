---
title: LangChain
---

In this tutorial, we'll walk through the steps to create a Chainlit application integrated with [LangChain](https://github.com/hwchase17/langchain).

<Frame caption="Preview of what you will build">
  <img src="/images/langchain-example.gif" />
</Frame>

## Prerequisites

Before getting started, make sure you have the following:

- A working installation of Chainlit
- The LangChain package installed
- An OpenAI API key
- Basic understanding of Python programming

## Step 1: Create a Python file

Create a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.

## Step 2: Write the Application Logic

In `app.py`, import the necessary packages and define one function to handle a new chat session and another function to handle messages incoming from the UI.

In this tutorial, we are going to use `LLMChain` to keep it simple. Here's the basic structure of the script:

<Note>
  If your agent/chain does not have an async implementation, fallback to the
  sync implementation.
</Note>

<CodeGroup>

```python Async
from langchain import PromptTemplate, OpenAI, LLMChain
import chainlit as cl
from chainlit.langchain.callbacks import AsyncLangchainCallbackHandler

template = """Question: {question}

Answer: Let's think step by step."""


@cl.on_chat_start
def main():
    # Instantiate the chain for that user session
    prompt = PromptTemplate(template=template, input_variables=["question"])
    llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)

    # Store the chain in the user session
    cl.user_session.set("llm_chain", llm_chain)


@cl.on_message
async def main(message: str):
    # Retrieve the chain from the user session
    llm_chain = cl.user_session.get("llm_chain")  # type: LLMChain

    # Call the chain asynchronously
    res = await llm_chain.acall(message, callbacks=[AsyncLangchainCallbackHandler()])

    # Do any post processing here

    # "res" is a Dict. For this chain, we get the response by reading the "text" key.
    # This varies from chain to chain, you should check which key to read.
    await cl.Message(content=res["text"]).send()
```

```python Sync
from langchain import PromptTemplate, OpenAI, LLMChain
import chainlit as cl
from chainlit.langchain.callbacks import LangchainCallbackHandler

template = """Question: {question}

Answer: Let's think step by step."""


@cl.on_chat_start
def main():
    # Instantiate the chain for that user session
    prompt = PromptTemplate(template=template, input_variables=["question"])
    llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)

    # Store the chain in the user session
    cl.user_session.set("llm_chain", llm_chain)


@cl.on_message
async def main(message: str):
    # Retrieve the chain from the user session
    llm_chain = cl.user_session.get("llm_chain")  # type: LLMChain

    # Call the chain synchronously in a different thread
    res = await cl.make_async(llm_chain)(
        message, callbacks=[LangchainCallbackHandler()]
    )

    # Do any post processing here

    # "res" is a Dict. For this chain, we get the response by reading the "text" key.
    # This varies from chain to chain, you should check which key to read.
    await cl.Message(content=res["text"]).send()
    return llm_chain
```

</CodeGroup>

This code sets up an instance of `LLMChain` with a custom `PromptTemplate` for each chat session. The `LLMChain` is invoked everytime a user sends a message to generate the response.

The callback handler is responsible for listening to the chain's intermediate steps and sending them to the UI.

## Step 3: Run the Application

To start your LLM app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:

```bash
chainlit run app.py -w
```

The `-w` flag tells Chainlit to enable auto-reloading, so you don't need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000.

<Warning>
  When using LangChain, prompts and completions are cached locally! The cache
  lives in the .chainlit folder. You can start the app with the `--no-cache`
  flag to disable it.
</Warning>

## Next Steps

Congratulations! You've just created your first LLM app with Chainlit and LangChain. From here, you can add [elements](/concepts/elements) and [actions](/concepts/actions) to create a more sophisticated app.

Happy coding! ðŸŽ‰
