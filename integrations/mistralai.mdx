---
title: Mistral AI
---

The benefits of this integration is that you can see the Mistral AI API calls in a step in the UI, and you can explore them in the prompt playground.

You will also get the full generation details (prompt, completion, tokens per second...) in your Literal AI dashboard, if your project is using [Literal AI](https://docs.literalai.com).

To benefit from tracing, you need to add `cl.instrument_mistralai()` after creating your Mistral AI client.

<Warning>
  You shouldn't configure this integration if you're already using another
  integration like Haystack, Langchain or LlamaIndex. Both integrations would
  record the same generation and create duplicate steps in the UI.
</Warning>

## Prerequisites

Before getting started, make sure you have the following:

- A working installation of Chainlit
- The Mistral AI python client package installed, `mistralai`
- A [Mistral AI API key](https://console.mistral.ai/api-keys/)
- Basic understanding of Python programming

## Step 1: Create a Python file

Create a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.

## Step 2: Write the Application Logic

In `app.py`, import the necessary packages and define one function to handle messages incoming from the UI.

```python
import os
import chainlit as cl

from mistralai.async_client import MistralAsyncClient

mai_client = MistralAsyncClient()

# Instrument the Mistral AI client
cl.instrument_mistralai()

settings = {
    "model": "mistral-large-latest",
    "temperature": 0,
    # ... more settings
}

@cl.on_message
async def on_message(message: cl.Message):
    response = await mai_client.chat(
        messages=[
            {
                "content": "You are a helpful bot, you always reply in Spanish",
                "role": "system"
            },
            {
                "content": message.content,
                "role": "user"
            }
        ],
        **settings
    )
    await cl.Message(content=response.choices[0].message.content).send()
```

## Step 3: Fill the environment variables

Create a file named `.env` in the same folder as your `app.py` file. Add your Mistral AI API key in the `MISTRAL_API_KEY` variable.
You can optionally add your Literal AI API key in the `LITERAL_API_KEY`.

## Step 4: Run the Application

To start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:

```bash
chainlit run app.py -w
```

The `-w` flag tells Chainlit to enable auto-reloading, so you don't need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000.
