---
title: Migration Guide to Chainlit v0.6.0
---

## Reasons to migrate

Chainlit v0.6.0 introduces several enhancements, such as:

- More streamlined and consistent APIs.
- Improved websocket reconnection mechanism.
- Support for Langchain final answer streaming.
- The ability to add custom endpoints to the Fast API server.

## Updating Chainlit

Begin the migration by updating Chainlit to the latest version:

```bash
pip install --upgrade chainlit
```

If you are using the [Local Persistence](/cloud/persistence/local) mode, you will need to run the following command to update the database schema:

```bash
chainlit migrate
```

## Breaking changes

To benefit from the new features and performance enhancements, migrate your code to the new APIs. Be aware of the following breaking changes:

### Removed APIs

The following APIs have been **removed**:

- `langchain_factory`
- `langchain_run`
- `langchain_postprocess`
- `llama_index_factory`
- `langflow_factory`

The factory concept was initially part of Chainlit. However, we found it was causing confusion and didn't add much value. As a result, we've removed it to simplify the API.

This doesn't mean you can't use LangChain, LangFlow, or LlamaIndex anymore. You'll just need to use the [on_chat_start](/api-reference/on-chat-start)/[on_message](/api-reference/on-message) pattern instead and pass the correct callback.

#### langchain_factory migration example

<CodeGroup>

```python Before
from langchain import PromptTemplate, OpenAI, LLMChain
import chainlit as cl

template = """Question: {question}

Answer: Let's think step by step."""

@cl.langchain_factory(use_async=True)
def factory():
    prompt = PromptTemplate(template=template, input_variables=["question"])
    llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)

    return llm_chain
```

```python After
from langchain import PromptTemplate, OpenAI, LLMChain
import chainlit as cl

template = """Question: {question}

Answer: Let's think step by step."""


@cl.on_chat_start
def main():
    # Instantiate the chain for that user session
    prompt = PromptTemplate(template=template, input_variables=["question"])
    llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)

    # Store the chain in the user session
    cl.user_session.set("llm_chain", llm_chain)


@cl.on_message
async def main(message: str):
    # Retrieve the chain from the user session
    llm_chain = cl.user_session.get("llm_chain")  # type: LLMChain

    # Call the chain asynchronously
    res = await llm_chain.acall(message, callbacks=[cl.AsyncLangchainCallbackHandler()])

    # Do any post processing here

    # Send the response
    await cl.Message(content=res["text"]).send()
```

</CodeGroup>

This example can be applied to any framework. Simply create a per-user instance in [on_chat_start](/api-reference/on-chat-start), store it in the [user_session](/concepts/user-session), and retrieve it in [on_message](/api-reference/on-message) to process the message.

<Note>
  You'll need to manually pass the correct callback when executing your chain or
  agent.
</Note>

### Updated APIs

The following APIs have been **updated**:

- [Message.update](/api-reference/message#update-a-message)
- `langchain_rename` becomes [author_rename](/api-reference/author-rename)

#### Example: Migrating from Message.update

The new approach to updating a message is simpler. Just update the attributes of the `Message` instance and call [Message.update](/api-reference/message#update-a-message) without any parameters.

<CodeGroup>

```python Before
import chainlit as cl


@cl.on_chat_start
async def main():
    msg = cl.Message(content="Hello!")
    await msg.send()
    await cl.sleep(2)
    await msg.update(content="Hello again!")
```

```python After
import chainlit as cl


@cl.on_chat_start
async def main():
    msg = cl.Message(content="Hello!")
    await msg.send()

    await cl.sleep(2)

    msg.content = "Hello again!"
    await msg.update()
```

</CodeGroup>

#### Example: Migrating from langchain_rename

`langchain_rename` is now [author_rename](/api-reference/author-rename) and can be used in any context, not just with Langchain.

<CodeGroup>

```python Before
import chainlit as cl

@cl.langchain_rename
def rename(orig_author: str):
    rename_dict = {
        "LLMMathChain": "Albert Einstein"
    }

    return rename_dict.get(orig_author, orig_author)
```

```python After
import chainlit as cl

@cl.author_rename
def rename(orig_author: str):
    rename_dict = {"LLMMathChain": "Albert Einstein", "Chatbot": "Assistant"}
    return rename_dict.get(orig_author, orig_author)
```

</CodeGroup>

## Deprecations

The `indent` attribute of the [Message](/api-reference/message) class has been deprecated. To nest messages, use the `parent_id` attribute instead.

<CodeGroup>

```python Before
import chainlit as cl

@cl.on_chat_start
async def start():
    await cl.Message(content="Hello!").send()
    await cl.Message(content="Hello nested!", indent=1).send()
```

```python After
import chainlit as cl

@cl.on_chat_start
async def start():
    id = await cl.Message(content="Hello!").send()
    await cl.Message(content="Hello nested!", parent_id=id).send()
```

</CodeGroup>

## Resources

The documentation, [examples](/examples), and [cookbook](https://github.com/Chainlit/cookbook) have been updated to reflect the new APIs. If you have any questions, don't hesitate to reach out on our [Discord](https://discord.gg/ZThrUxbAYw) server.
