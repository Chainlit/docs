---
title: Migration Guide to Chainlit v0.3.0
---

## Reasons to migrate

Chainlit v0.3.0 brings a range of enhancements, including bug fixes, performance improvements, and new features. Some key improvements are:

- The adoption of an asynchronous architecture, which results in better performance and compatibility with the async Python ecosystem.
- A smaller memory footprint, faster streaming, and support for concurrent tasks.
- A cleaner, more maintainable codebase with the removal of multiple workarounds, leading to a more stable installation process.

## New features

- The [Avatar](/api-reference/elements/avatar) element for displaying an avatar image next to a message.
- [AskFileMessage](/api-reference/ask/ask-for-file) now supports multiple file uploads. (See breaking changes below)
- A new user menu in the UI, offering settings to expand or retract messages for user convenience.

## Updating Chainlit and LangChain

Begin the migration by updating Chainlit and LangChain (if you use it) to the latest version:

```bash
pip install --upgrade chainlit langchain
```

## Breaking changes

To benefit from the new features and performance enhancements, migrate your code to the new APIs. Be aware of the following breaking changes:

### Asynchronous APIs

These changes affect [messages](http://localhost:3000/api-reference/message), [elements](/api-reference/elements/), [actions](/api-reference/action/), and [asks](http://localhost:3000/api-reference/ask).

Add the `await` keyword in front of calls to these class methods, such as `.send`, and wrap them in an `async` function.

For example, observe the changes below:

<CodeGroup>

```python Before
import chainlit as cl

@cl.on_message
def main(message: str):
    cl.Message(author="Tool 1", content=f"Response from tool1", indent=1).send()
    cl.Message(content=f"This is the final answer").send()
```

```python After
import chainlit as cl

@cl.on_message
async def main(message: str):
    await cl.Message(author="Tool 1", content=f"Response from tool1", indent=1).send()
    await cl.Message(content=f"This is the final answer").send()
```

</CodeGroup>

Now there's an `async` keyword before the function definition and an `await` keyword in front of calls to `Message.send()`.

### langchain_factory

The [langchain_factory](/api-reference/langchain/langchain-factory) decorator requires a `use_async` argument to determine whether to use the async (`agent.acall()`) or sync (`agent()`) agent implementation. If synchronous implementation is used, the agent runs in a separate thread, preventing the event loop from blocking.

<CodeGroup>

```python Async
from langchain import OpenAI, LLMMathChain
import chainlit as cl

@cl.langchain_factory(use_async=True)
def factory():
    llm = OpenAI(temperature=0)
    llm_math = LLMMathChain.from_llm(llm=llm)
    return llm_math
```

```python Sync
from langchain import OpenAI, LLMMathChain
import chainlit as cl

@cl.langchain_factory(use_async=False)
def factory():
    llm = OpenAI(temperature=0)
    llm_math = LLMMathChain.from_llm(llm=llm)
    return llm_math
```

</CodeGroup>

### LangChain Callback Handler

Since Chainlit's launch, LangChain has improved its callback handling, eliminating the need to patch the LangChain package. This update means you'll need to pass the appropriate callback handler to your LangChain agent, as described [here](/api-reference/langchain/callbacks). If you're using the [langchain_factory](/api-reference/langchain/langchain-factory) decorator, this is done for you.

### langchain_run

When using [langchain_run](/api-reference/langchain/langchain-run) to run the agent yourself, opt for the async version, `agent.acall()`, if possible.

```python
@cl.langchain_run
async def run(agent, input_str):
    res = await agent.acall(input_str, callbacks=[cl.AsyncChainlitCallbackHandler()])
    await cl.Message(content=res["text"]).send()
```

If the agent doesn't have an async implementation, use [make_async](/api-reference/make-async) to run the agent in a different thread.

```python
@cl.langchain_run
async def run(agent, input_str):
    res = await cl.make_async(agent)(input_str, callbacks=[cl.ChainlitCallbackHandler()])
    await cl.Message(content=res["text"]).send()
```

### Text Element

For consistency with other elements, the [Text Element](/api-reference/elements/text) now requires a `content` argument rather than a `text` argument.

```python
@cl.on_message
async def main(message: str):
    elements = [
        cl.Text(content="This is a text element"),
    ]
    await cl.Message(content="Hello world", elements=elements).send()
```

### LocalImage/RemoteImage

Both `LocalImage` and `RemoteImage` have been combined into the [Image](/api-reference/elements/image) for a simpler API.

```python
@cl.on_message
async def main(message: str):
    elements = [
        cl.Image(name="Image from bytes", content=image_bytes),
        cl.Image(name="Image from URL", url="https://..."),
        cl.Image(name="Image from local path", path="./cat.jpg"),
    ]
    await cl.Message(content="Hello world", elements=elements).send()
```

### AskFileMessage

With the additional optional `max_files` parameter [AskFileMessage](/api-reference/ask/ask-for-file) now supports multiple file uploads. The returned type is a list of `AskFileResponse`.

```python
@cl.on_chat_start
async def start():
    files = await cl.AskFileMessage(
        content="Please upload a text file to begin!",
        max_files=2,
        accept={"text/plain": [".py"]},
    ).send()

    file_names = [file.name for file in files]

    await cl.Message(
        content=f"{len(files)} files uploaded: {','.join(file_names)}"
    ).send()
```

## Resources

The documentation, [examples](/examples), and [cookbook](https://github.com/Chainlit/cookbook) have been updated to reflect the new APIs. If you have any questions, don't hesitate to reach out on our [Discord](https://discord.gg/ZThrUxbAYw) server.
