---
title: Guide to v0.3.0
---

## Reasons to migrate

Chainlit v0.3.0 brings several enhancements, including bug fixes, performance improvements, and new features.

The most notable improvement is the adoption of an asynchronous architecture, resulting in significant performance gains and better compatibility with the async Python ecosystem.

Moreover, the codebase is cleaner and more maintainable with the removal of multiple monkey-patching hacks and other workarounds, leading to a more stable installation process.

## Update Chainlit and LangChain

To begin migration, update Chainlit and LangChain (if you use it) to the latest version.

```bash
pip install --upgrade chainlit langchain
```

## New features

- The [Avatar](/api-reference/elements/avatar) element allows for the display of an avatar image beside a message.
- [AskFileMessage](/api-reference/ask/ask-for-file) now supports multiple file uploads (refer to the breaking changes for more information).
- The UI features a new user menu containing a settings list, allowing users to expand or retract messages as needed.

## Breaking changes

To take advantage of the new features and performance enhancements, migrate your code to the new APIs. While this process should be straightforward, be aware of the following breaking changes:

### Asynchronous APIs

These changes impact [messages](http://localhost:3000/api-reference/message), [elements](/api-reference/elements/), [actions](/api-reference/action/), and [asks](http://localhost:3000/api-reference/ask).

Add an `await` in front of calls to these classes and wrap them in an `async` function.

Here's an example demonstrating the changes:

```python Before
import chainlit as cl

@cl.on_message  # this function is called every time a user inputs a message in the UI
def main(message: str):
    cl.Message(author="Tool 1", content=f"Response from tool1", indent=1).send()
    cl.Message(content=f"This is the final answer").send()
```

The updated version:

```python After
import chainlit as cl

@cl.on_message  # this function is called every time a user inputs a message in the UI
async def main(message: str):
    await cl.Message(author="Tool 1", content=f"Response from tool1", indent=1).send()
    await cl.Message(content=f"This is the final answer").send()
```

Notice the `async` keyword before the function definition, and the `await` keyword in front of calls to `Message.send()`.

### langchain_factory

The [langchain_factory](/api-reference/langchain-factory) decorator now requires a `use_async` argument, determining whether to use the async (`agent.acall()`) or sync (`agent()`) agent implementation. If the synchronous implementation is used, the agent runs in a separate thread to avoid blocking the event loop.

For example:

```python
from langchain import OpenAI, LLMMathChain
from chainlit import langchain_factory

@langchain_factory(use_async=True)
def factory():
    llm = OpenAI(temperature=0)
    llm_math = LLMMathChain.from_llm(llm=llm)
    return llm_math
```

### langchain_run

If using [langchain_run](/api-reference/langchain-run) to run the agent yourself, use the asynchronous version, `agent.acall()`, if possible.

```python
@cl.langchain_run
async def run(agent, input_str):
    res = await agent.acall(input_str, callbacks=[cl.AsyncChainlitCallbackHandler()])
    await cl.Message(content=res["text"]).send()
```

If the agent doesn't have an async implementation, use [asyncify](/api-reference/asyncify) to run the agent in a different thread.

```python
@cl.langchain_run
async def run(agent, input_str):
    res = await cl.asyncify(agent)(input_str, callbacks=[cl.ChainlitCallbackHandler()])
    await cl.Message(content=res["text"]).send()
```

### Text Element

The [Text Element](/api-reference/elements/text) now requires a `content` argument instead of a `text` argument for consistency with other elements.

```python
@cl.on_message
async def main(message: str):
    elements = [
        cl.Text(content="This is a text element"),
    ]
    await cl.Message(content="Hello world", elements=elements).send()
```

### LocalImage/RemoteImage

`LocalImage` and `RemoteImage` have been combined into [Image](/api-reference/elements/image) for a more straightforward API.

```python
@cl.on_message
async def main(message: str):
    elements = [
        cl.Image(name="Image from bytes", content=image_bytes),
        cl.Image(name="Image from URL", url="https://..."),
        cl.Image(name="Image from local path", path="./cat.jpg"),
    ]
    await cl.Message(content="Hello world", elements=elements).send()
```

### AskFileMessage

[AskFileMessage](/api-reference/ask/ask-for-file) now has an optional `max_files` parameter enabling multiple file uploads. The returned type is a list of `AskFileResponse`.

```python
@cl.on_chat_start
async def start():
    files = await cl.AskFileMessage(
        content="Please upload a text file to begin!",
        max_files=2,
        accept={"text/plain": [".py"]},
    ).send()

    file_names = [file.name for file in files]

    await cl.Message(
        content=f"{len(files)} files uploaded: {','.join(file_names)}"
    ).send()
```

## Resources

The documentation, [examples](/examples), and [cookbook](https://github.com/Chainlit/cookbook) have all been updated to reflect the new APIs. If you have any questions, feel free to reach out on our [Discord](https://discord.gg/ZThrUxbAYw) server.
